{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874e3db1",
   "metadata": {},
   "source": [
    "# Get Reference LD Panel\n",
    "This notebook calculates the Linkage Disequilibrium (LD) matrix for a specific set of SNPs using reference genotypes from the 1000 Genomes Project (Phase 3).\n",
    "\n",
    "**Features:**\n",
    "- Uses **EUR** (European) samples only.\n",
    "- Streams data directly from 1000 Genomes FTP (via `pysam`) to minimize disk usage.\n",
    "- Calculates pairwise correlation (R) matrix.\n",
    "- Saves the LD matrix and corresponding SNP metadata.\n",
    "\n",
    "**Requirements:**\n",
    "- `pysam` (must be installed with libcurl support for remote file access)\n",
    "- `pandas`, `numpy`\n",
    "\n",
    "**Note for Windows Users:**\n",
    "`pysam` is notoriously difficult to install on Windows because it requires compiling C extensions (htslib). If you cannot install `pysam`, you may need to use **WSL (Windows Subsystem for Linux)** or a Docker container. Alternatively, you can try `cyvcf2` if a wheel is available, but `pysam` is the standard for remote VCF access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd163e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpysam\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Determine project root relative to this notebook\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pysam'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pysam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Determine project root relative to this notebook\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Add project root to sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Pysam version: {pysam.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "GENE_NAME = \"CBX8\"\n",
    "POPULATION = \"EUR\"  # Super-population code (EUR, AFR, EAS, SAS, AMR)\n",
    "\n",
    "# Input VCF containing the SNPs of interest\n",
    "# This file was generated by get_z_scores.ipynb\n",
    "INPUT_VCF_PATH = PROJECT_ROOT / \"output\" / f\"{GENE_NAME}_variants.vcf\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "LD_MATRIX_PATH = OUTPUT_DIR / f\"{GENE_NAME}_LD_matrix.csv\"\n",
    "LD_SNPS_PATH = OUTPUT_DIR / f\"{GENE_NAME}_LD_snps.csv\"\n",
    "\n",
    "# 1000 Genomes Phase 3 URL Template\n",
    "# We will fill in the chromosome dynamically\n",
    "KG_URL_TEMPLATE = \"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr{chrom}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "# Use the standard panel file (TSV) instead of the Excel file for better reliability and fewer dependencies\n",
    "KG_SAMPLE_INFO_URL = \"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\"\n",
    "\n",
    "print(f\"Target Gene: {GENE_NAME}\")\n",
    "print(f\"Population: {POPULATION}\")\n",
    "print(f\"Input VCF: {INPUT_VCF_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Target SNPs\n",
    "print(\"Loading target SNPs from local VCF...\")\n",
    "if not INPUT_VCF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Input VCF not found: {INPUT_VCF_PATH}\")\n",
    "\n",
    "# We can use pandas to read the VCF (skipping header lines)\n",
    "# The VCF from get_z_scores.ipynb has a header line starting with #CHROM\n",
    "target_snps = pd.read_csv(INPUT_VCF_PATH, sep=\"\\t\", comment=\"#\", header=None, names=[\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"])\n",
    "\n",
    "# If the file has a proper header row that wasn't commented out correctly in previous steps, we might need to adjust.\n",
    "# Let's check if the first row looks like a header or data.\n",
    "# Actually, standard VCFs have #CHROM. pandas comment='#' will skip it.\n",
    "# We need to read the header separately or just assume columns.\n",
    "# Let's try reading with comment='#' and manually assigning columns, assuming standard format.\n",
    "\n",
    "# Re-read to be safe about header\n",
    "with open(INPUT_VCF_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#CHROM\"):\n",
    "            header = line.strip().replace(\"#\", \"\").split(\"\\t\")\n",
    "            break\n",
    "    else:\n",
    "        header = [\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]\n",
    "\n",
    "target_snps = pd.read_csv(INPUT_VCF_PATH, sep=\"\\t\", comment=\"#\", names=header)\n",
    "\n",
    "# Ensure CHROM is string and doesn't have 'chr' prefix if 1kGP doesn't use it (1kGP usually uses numbers, but URLs might need adjustment)\n",
    "# 1kGP VCFs usually have just numbers \"1\", \"2\", ... in the file, but filenames might be \"chr1\".\n",
    "target_snps['CHROM'] = target_snps['CHROM'].astype(str).str.replace(\"chr\", \"\")\n",
    "\n",
    "print(f\"Loaded {len(target_snps)} SNPs.\")\n",
    "target_snps.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Reference Sample Info\n",
    "print(\"Downloading 1000 Genomes sample info...\")\n",
    "\n",
    "# Read the panel file (TSV)\n",
    "# Columns are: sample, pop, super_pop, gender\n",
    "sample_info = pd.read_csv(KG_SAMPLE_INFO_URL, sep='\\t')\n",
    "\n",
    "# Filter for Population\n",
    "eur_samples = sample_info[sample_info['super_pop'] == POPULATION]['sample'].tolist()\n",
    "\n",
    "print(f\"Found {len(eur_samples)} samples for super-population {POPULATION}\")\n",
    "print(f\"First 5 samples: {eur_samples[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5482bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fetch Genotypes\n",
    "# We will iterate through the target SNPs and fetch genotypes from the remote VCF.\n",
    "# To be efficient, we'll group by chromosome (though likely only one gene/chrom here).\n",
    "\n",
    "chromosomes = target_snps['CHROM'].unique()\n",
    "genotypes_list = []\n",
    "found_snps_list = []\n",
    "\n",
    "for chrom in chromosomes:\n",
    "    vcf_url = KG_URL_TEMPLATE.format(chrom=chrom)\n",
    "    print(f\"Connecting to {vcf_url}...\")\n",
    "    \n",
    "    try:\n",
    "        # Open remote VCF\n",
    "        # pysam allows opening URLs if libcurl is enabled\n",
    "        vcf_in = pysam.VariantFile(vcf_url)\n",
    "        \n",
    "        # Get sample indices for our population\n",
    "        # vcf_in.header.samples is a list of all samples in the VCF\n",
    "        vcf_samples = list(vcf_in.header.samples)\n",
    "        sample_indices = [i for i, s in enumerate(vcf_samples) if s in eur_samples]\n",
    "        \n",
    "        if not sample_indices:\n",
    "            raise ValueError(f\"No matching samples found in VCF for population {POPULATION}\")\n",
    "            \n",
    "        print(f\"  Extracting genotypes for {len(sample_indices)} samples...\")\n",
    "        \n",
    "        # Iterate over target SNPs on this chromosome\n",
    "        chrom_snps = target_snps[target_snps['CHROM'] == chrom]\n",
    "        \n",
    "        for _, row in tqdm(chrom_snps.iterrows(), total=len(chrom_snps), desc=f\"Chr {chrom}\"):\n",
    "            pos = row['POS']\n",
    "            ref = row['REF']\n",
    "            alt = row['ALT']\n",
    "            rsid = row['ID']\n",
    "            \n",
    "            # Fetch records at this position\n",
    "            # Note: pysam fetch is 0-based? No, fetch(region) uses 1-based coordinates usually, or start/end.\n",
    "            # fetch(contig, start, stop) is 0-based, half-open.\n",
    "            # VCF POS is 1-based. So start=pos-1, stop=pos.\n",
    "            \n",
    "            try:\n",
    "                # We use a small window to be safe, but really just want the exact pos\n",
    "                records = list(vcf_in.fetch(chrom, pos-1, pos))\n",
    "            except ValueError as e:\n",
    "                # Contig not found or other error\n",
    "                print(f\"  Error fetching {chrom}:{pos}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            # Find the matching record\n",
    "            match = None\n",
    "            for rec in records:\n",
    "                # Check position (rec.pos is 1-based)\n",
    "                if rec.pos != pos:\n",
    "                    continue\n",
    "                    \n",
    "                # Check Ref/Alt\n",
    "                # 1kGP might have multiallelics.\n",
    "                # We need to find if our Alt is in the record's Alts.\n",
    "                if rec.ref != ref:\n",
    "                    # Possible strand flip? For now, skip if mismatch.\n",
    "                    # GTEx and 1kGP are both usually + strand hg38.\n",
    "                    continue\n",
    "                \n",
    "                if alt in rec.alts:\n",
    "                    match = rec\n",
    "                    alt_index = rec.alts.index(alt) + 1 # 1-based index for GT\n",
    "                    break\n",
    "            \n",
    "            if match:\n",
    "                # Extract genotypes\n",
    "                # rec.samples is a proxy object, iterating it is slow.\n",
    "                # We can use specialized methods if available, but standard pysam is:\n",
    "                # [s['GT'] for s in rec.samples.values()]\n",
    "                # But we only want specific samples.\n",
    "                \n",
    "                # Optimization: subsetting samples is slow in python loop.\n",
    "                # Faster approach: get all GTs, then numpy slice.\n",
    "                # But pysam doesn't give numpy array directly easily without iterating.\n",
    "                # Let's just iterate the subset of samples we care about.\n",
    "                \n",
    "                # Actually, iterating 500 samples per SNP for 2000 SNPs is 1M ops. Fast enough.\n",
    "                \n",
    "                row_gts = []\n",
    "                for samp_idx in sample_indices:\n",
    "                    # Get sample name\n",
    "                    samp_name = vcf_samples[samp_idx]\n",
    "                    # Get GT\n",
    "                    gt = match.samples[samp_name]['GT']\n",
    "                    # gt is tuple like (0, 1)\n",
    "                    \n",
    "                    # Convert to dosage (0, 1, 2)\n",
    "                    # None means missing\n",
    "                    if gt[0] is None or gt[1] is None:\n",
    "                        dosage = np.nan\n",
    "                    else:\n",
    "                        # Check if alleles match our target alt\n",
    "                        # We are looking for dosage of 'alt'.\n",
    "                        # alt_index is the index of our alt allele in the VCF record.\n",
    "                        # 0 is ref.\n",
    "                        dosage = sum(1 for allele in gt if allele == alt_index)\n",
    "                    \n",
    "                    row_gts.append(dosage)\n",
    "                \n",
    "                genotypes_list.append(row_gts)\n",
    "                found_snps_list.append(row)\n",
    "            else:\n",
    "                # SNP not found in reference\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process chromosome {chrom}: {e}\")\n",
    "\n",
    "print(f\"Extracted genotypes for {len(found_snps_list)} / {len(target_snps)} SNPs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0479a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate LD Matrix\n",
    "if not genotypes_list:\n",
    "    raise ValueError(\"No genotypes extracted. Cannot calculate LD.\")\n",
    "\n",
    "# Convert to numpy array\n",
    "# Shape: (n_snps, n_samples)\n",
    "G = np.array(genotypes_list)\n",
    "\n",
    "# Check for missing data\n",
    "if np.isnan(G).any():\n",
    "    print(\"Warning: Missing genotypes detected. Imputing with mean...\")\n",
    "    # Simple mean imputation for LD calculation\n",
    "    row_means = np.nanmean(G, axis=1)\n",
    "    inds = np.where(np.isnan(G))\n",
    "    G[inds] = np.take(row_means, inds[0])\n",
    "\n",
    "print(f\"Genotype matrix shape: {G.shape}\")\n",
    "\n",
    "# Calculate Correlation Matrix (R)\n",
    "# np.corrcoef expects each row to be a variable (SNP), so shape (n_snps, n_samples) is correct.\n",
    "ld_matrix = np.corrcoef(G)\n",
    "\n",
    "print(f\"LD Matrix shape: {ld_matrix.shape}\")\n",
    "print(\"LD calculation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbece71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Results\n",
    "\n",
    "# Save SNP metadata for the rows/cols of the matrix\n",
    "ld_snps_df = pd.DataFrame(found_snps_list)\n",
    "ld_snps_df.to_csv(LD_SNPS_PATH, index=False)\n",
    "print(f\"Saved SNP metadata to: {LD_SNPS_PATH}\")\n",
    "\n",
    "# Save LD Matrix\n",
    "# We'll save as CSV for simplicity, though .npy or .parquet is better for large matrices.\n",
    "# Given ~2000 SNPs, CSV is fine (~4M entries).\n",
    "ld_df = pd.DataFrame(ld_matrix, index=ld_snps_df['ID'], columns=ld_snps_df['ID'])\n",
    "ld_df.to_csv(LD_MATRIX_PATH)\n",
    "print(f\"Saved LD matrix to: {LD_MATRIX_PATH}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFirst 5x5 LD Matrix:\")\n",
    "print(ld_df.iloc[:5, :5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_py_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

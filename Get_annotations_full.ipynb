{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Borzoi Variant Effect Prediction on Gene Expression\n",
    "\n",
    "This notebook performs in silico mutagenesis to predict the effect of SNPs on gene expression.\n",
    "\n",
    "**Features:**\n",
    "- Configurable task filtering (assay, description, name keywords)\n",
    "- Both TSS-centered and SNP-centered prediction strategies\n",
    "- Averaging across all 4 Borzoi replicates\n",
    "- Comprehensive validation checks\n",
    "- Z-score normalization of effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Package dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable W&B\n",
    "# os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Function redefined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# USER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Gene of interest\n",
    "GENE_NAME = \"CBX8\"  # Change this to your gene of interest\n",
    "\n",
    "# VCF file path (set to your VCF file)\n",
    "VCF_FILE = \"path/to/your/variants.vcf\"  # UPDATE THIS PATH\n",
    "GTF_CACHE_DIR = \"/storage/home/mgc5166/work/Annotations/eQTL_annotations_for_susine/data/gtf_cache\"\n",
    "\n",
    "# Task filtering (case-insensitive, uses 'contains')\n",
    "# Set to None to skip that filter\n",
    "TASK_FILTER = {\n",
    "    \"assay\": \"rna\",           # e.g., \"rna\", \"cage\", \"atac\"\n",
    "    \"description\": \"lung\",    # e.g., \"lung\", \"brain\", \"liver\"\n",
    "    \"name\": \"gtex\",           # e.g., \"gtex\", \"encode\"\n",
    "}\n",
    "\n",
    "# Aggregation method for predictions across tasks and bins\n",
    "TASK_AGGFUNC = \"mean\"    # \"mean\" or \"sum\"\n",
    "LENGTH_AGGFUNC = \"sum\"   # \"mean\" or \"sum\" (sum is typical for gene expression)\n",
    "\n",
    "# Model replicates to use (0, 1, 2, 3 available)\n",
    "MODEL_REPLICATES = [0, 1, 2, 3]\n",
    "\n",
    "# Set device to GPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def predict_on_sequence(\n",
    "    model,\n",
    "    seq: str,\n",
    "    device: str = DEVICE\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run model prediction on a single sequence.\n",
    "    Returns array of shape (1, n_tasks, n_bins).\n",
    "    \"\"\"\n",
    "    preds = model.predict_on_seqs([seq], device=device)  # Fixed: devices -> device\n",
    "    return preds\n",
    "\n",
    "print(\"Function redefined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gtf-header",
   "metadata": {},
   "source": [
    "## 3. Download/Load GTF Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gtf-download",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 94\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m genes_df, exons_df\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Download and load GTF\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m gtf_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_gtf_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGTF_CACHE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m genes_df, exons_df \u001b[38;5;241m=\u001b[39m load_gene_annotations(gtf_path)\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mdownload_gtf_if_needed\u001b[0;34m(cache_dir, genome)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_gtf_if_needed\u001b[39m(cache_dir: Path, genome: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhg38\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Download GENCODE GTF file if not present locally.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Returns path to the GTF file.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     gtf_path \u001b[38;5;241m=\u001b[39m \u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgenome\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_gencode.gtf.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gtf_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTF file already exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgtf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "def download_gtf_if_needed(cache_dir: Path, genome: str = \"hg38\") -> Path:\n",
    "    \"\"\"\n",
    "    Download GENCODE GTF file if not present locally.\n",
    "    Returns path to the GTF file.\n",
    "    \"\"\"\n",
    "    gtf_path = cache_dir / f\"{genome}_gencode.gtf.gz\"\n",
    "    \n",
    "    if gtf_path.exists():\n",
    "        print(f\"GTF file already exists: {gtf_path}\")\n",
    "        return gtf_path\n",
    "    \n",
    "    print(\"Downloading GENCODE GTF file...\")\n",
    "    \n",
    "    # GENCODE v44 for hg38\n",
    "    url = \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\n",
    "    \n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url, gtf_path)\n",
    "    \n",
    "    print(f\"Downloaded GTF to: {gtf_path}\")\n",
    "    return gtf_path\n",
    "\n",
    "\n",
    "def load_gene_annotations(gtf_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load gene annotations from GTF file.\n",
    "    Returns DataFrame with gene info including TSS.\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    \n",
    "    print(\"Loading GTF file (this may take a moment)...\")\n",
    "    \n",
    "    genes = []\n",
    "    exons = []\n",
    "    \n",
    "    opener = gzip.open if str(gtf_path).endswith('.gz') else open\n",
    "    \n",
    "    with opener(gtf_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) < 9:\n",
    "                continue\n",
    "            \n",
    "            chrom, source, feature, start, end, score, strand, frame, attributes = fields\n",
    "            \n",
    "            # Parse attributes\n",
    "            attr_dict = {}\n",
    "            for attr in attributes.split(';'):\n",
    "                attr = attr.strip()\n",
    "                if attr:\n",
    "                    parts = attr.split(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        key, value = parts\n",
    "                        attr_dict[key] = value.strip('\"')\n",
    "            \n",
    "            gene_name = attr_dict.get('gene_name', '')\n",
    "            gene_type = attr_dict.get('gene_type', '')\n",
    "            \n",
    "            if feature == 'gene':\n",
    "                genes.append({\n",
    "                    'chrom': chrom,\n",
    "                    'start': int(start) - 1,  # Convert to 0-based\n",
    "                    'end': int(end),\n",
    "                    'gene_name': gene_name,\n",
    "                    'gene_type': gene_type,\n",
    "                    'strand': strand,\n",
    "                })\n",
    "            elif feature == 'exon':\n",
    "                exons.append({\n",
    "                    'chrom': chrom,\n",
    "                    'start': int(start) - 1,\n",
    "                    'end': int(end),\n",
    "                    'gene_name': gene_name,\n",
    "                    'strand': strand,\n",
    "                })\n",
    "    \n",
    "    genes_df = pd.DataFrame(genes)\n",
    "    exons_df = pd.DataFrame(exons)\n",
    "    \n",
    "    # Add TSS (start for + strand, end for - strand)\n",
    "    genes_df['tss'] = genes_df.apply(\n",
    "        lambda row: row['start'] if row['strand'] == '+' else row['end'] - 1,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(genes_df)} genes and {len(exons_df)} exons\")\n",
    "    return genes_df, exons_df\n",
    "\n",
    "\n",
    "# Download and load GTF\n",
    "gtf_path = download_gtf_if_needed(GTF_CACHE_DIR)\n",
    "genes_df, exons_df = load_gene_annotations(gtf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-gene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gene: CBX8\n",
      "  Location: chr17:79792131-79801683\n",
      "  Strand: -\n",
      "  TSS: 79801682\n",
      "  Number of exons: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79796929</td>\n",
       "      <td>79797077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79796496</td>\n",
       "      <td>79796540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79796249</td>\n",
       "      <td>79796315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79796056</td>\n",
       "      <td>79796123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79792131</td>\n",
       "      <td>79795558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom     start       end\n",
       "0  chr17  79796929  79797077\n",
       "1  chr17  79796496  79796540\n",
       "2  chr17  79796249  79796315\n",
       "3  chr17  79796056  79796123\n",
       "4  chr17  79792131  79795558"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gene_info(gene_name: str, genes_df: pd.DataFrame, exons_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Get gene information including TSS and exon coordinates.\n",
    "    \"\"\"\n",
    "    # Find gene (case-insensitive)\n",
    "    gene_mask = genes_df['gene_name'].str.upper() == gene_name.upper()\n",
    "    \n",
    "    if not gene_mask.any():\n",
    "        raise ValueError(f\"Gene '{gene_name}' not found in annotations\")\n",
    "    \n",
    "    gene_info = genes_df[gene_mask].iloc[0]\n",
    "    \n",
    "    # Get exons for this gene\n",
    "    gene_exons = exons_df[exons_df['gene_name'].str.upper() == gene_name.upper()].copy()\n",
    "    gene_exons = gene_exons.drop_duplicates(subset=['chrom', 'start', 'end'])\n",
    "    \n",
    "    result = {\n",
    "        'gene_name': gene_info['gene_name'],\n",
    "        'chrom': gene_info['chrom'],\n",
    "        'start': gene_info['start'],\n",
    "        'end': gene_info['end'],\n",
    "        'strand': gene_info['strand'],\n",
    "        'tss': gene_info['tss'],\n",
    "        'exons': gene_exons[['chrom', 'start', 'end']].reset_index(drop=True),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nGene: {result['gene_name']}\")\n",
    "    print(f\"  Location: {result['chrom']}:{result['start']}-{result['end']}\")\n",
    "    print(f\"  Strand: {result['strand']}\")\n",
    "    print(f\"  TSS: {result['tss']}\")\n",
    "    print(f\"  Number of exons: {len(result['exons'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Get info for our gene of interest\n",
    "gene_info = get_gene_info(GENE_NAME, genes_df, exons_df)\n",
    "gene_info['exons'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 4. Load Borzoi Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous parameter to wandb.login() has no effect and will be removed in future versions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Borzoi human_rep0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgc5166\u001b[0m (\u001b[33mmgc5166-penn-state\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'human_rep0:latest', 711.80MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:01.5 (477.4MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Borzoi human_rep1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'human_rep1:latest', 711.80MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:11.4 (62.4MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Borzoi human_rep2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'human_rep2:latest', 711.80MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:11.0 (64.5MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Borzoi human_rep3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 333648 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'human_rep3:latest', 711.80MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:10.3 (69.2MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 4 model replicates\n"
     ]
    }
   ],
   "source": [
    "import grelu.resources\n",
    "\n",
    "def load_borzoi_replicate(rep: int):\n",
    "    \"\"\"Load a single Borzoi replicate model.\"\"\"\n",
    "    print(f\"Loading Borzoi human_rep{rep}...\")\n",
    "    model = grelu.resources.load_model(\n",
    "        project=\"borzoi\",\n",
    "        model_name=f\"human_rep{rep}\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load all replicates\n",
    "models = {}\n",
    "for rep in MODEL_REPLICATES:\n",
    "    models[rep] = load_borzoi_replicate(rep)\n",
    "\n",
    "# Use first model for parameters\n",
    "model = models[MODEL_REPLICATES[0]]\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} model replicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-params",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data parameters:\n",
      "  seq_len: 524288\n",
      "  label_len: 196608\n",
      "  genome: hg38\n",
      "  bin_size: 32\n",
      "\n",
      "Key dimensions:\n",
      "  Input sequence length: 524,288 bp\n",
      "  Output length: 196,608 bp\n",
      "  Bin size: 32 bp\n",
      "  Crop length: 5120 bins (163,840 bp)\n",
      "  Editable region: 163,840 - 360,448 (196,608 bp)\n"
     ]
    }
   ],
   "source": [
    "# Display model parameters\n",
    "print(\"Model data parameters:\")\n",
    "for key in model.data_params['train'].keys():\n",
    "    if key != \"intervals\":\n",
    "        print(f\"  {key}: {model.data_params['train'][key]}\")\n",
    "\n",
    "# Key parameters\n",
    "SEQ_LEN = model.data_params[\"train\"][\"seq_len\"]  # 524288\n",
    "LABEL_LEN = model.data_params[\"train\"][\"label_len\"]  # 196608\n",
    "BIN_SIZE = model.data_params[\"train\"][\"bin_size\"]  # 32\n",
    "CROP_LEN = model.model_params.get(\"crop_len\", 0)  # Cropped bins on each side\n",
    "\n",
    "# Calculate the editable region (input positions that affect output)\n",
    "CROP_BP = CROP_LEN * BIN_SIZE\n",
    "EDITABLE_START = CROP_BP  # First position in input that produces output\n",
    "EDITABLE_END = SEQ_LEN - CROP_BP  # Last position\n",
    "\n",
    "print(f\"\\nKey dimensions:\")\n",
    "print(f\"  Input sequence length: {SEQ_LEN:,} bp\")\n",
    "print(f\"  Output length: {LABEL_LEN:,} bp\")\n",
    "print(f\"  Bin size: {BIN_SIZE} bp\")\n",
    "print(f\"  Crop length: {CROP_LEN} bins ({CROP_BP:,} bp)\")\n",
    "print(f\"  Editable region: {EDITABLE_START:,} - {EDITABLE_END:,} ({EDITABLE_END - EDITABLE_START:,} bp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tasks-header",
   "metadata": {},
   "source": [
    "## 5. Filter Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-tasks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 3 tasks\n",
      "\n",
      "Sample of filtered tasks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>file</th>\n",
       "      <th>clip</th>\n",
       "      <th>clip_soft</th>\n",
       "      <th>scale</th>\n",
       "      <th>sum_stat</th>\n",
       "      <th>strand_pair</th>\n",
       "      <th>description</th>\n",
       "      <th>assay</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>GTEX-1399S-1726-SM-5L3DI.1</td>\n",
       "      <td>/home/drk/tillage/datasets/human/rna/recount3/...</td>\n",
       "      <td>768</td>\n",
       "      <td>384</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sum_sqrt</td>\n",
       "      <td>7566</td>\n",
       "      <td>RNA:lung</td>\n",
       "      <td>RNA</td>\n",
       "      <td>lung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>GTEX-14AS3-0926-SM-5TDD6.1</td>\n",
       "      <td>/home/drk/tillage/datasets/human/rna/recount3/...</td>\n",
       "      <td>768</td>\n",
       "      <td>384</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sum_sqrt</td>\n",
       "      <td>7567</td>\n",
       "      <td>RNA:lung</td>\n",
       "      <td>RNA</td>\n",
       "      <td>lung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>GTEX-14JG1-0926-SM-5YY8W.1</td>\n",
       "      <td>/home/drk/tillage/datasets/human/rna/recount3/...</td>\n",
       "      <td>768</td>\n",
       "      <td>384</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sum_sqrt</td>\n",
       "      <td>7568</td>\n",
       "      <td>RNA:lung</td>\n",
       "      <td>RNA</td>\n",
       "      <td>lung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name  \\\n",
       "7566  GTEX-1399S-1726-SM-5L3DI.1   \n",
       "7567  GTEX-14AS3-0926-SM-5TDD6.1   \n",
       "7568  GTEX-14JG1-0926-SM-5YY8W.1   \n",
       "\n",
       "                                                   file  clip  clip_soft  \\\n",
       "7566  /home/drk/tillage/datasets/human/rna/recount3/...   768        384   \n",
       "7567  /home/drk/tillage/datasets/human/rna/recount3/...   768        384   \n",
       "7568  /home/drk/tillage/datasets/human/rna/recount3/...   768        384   \n",
       "\n",
       "      scale  sum_stat  strand_pair description assay sample  \n",
       "7566   0.01  sum_sqrt         7566    RNA:lung   RNA   lung  \n",
       "7567   0.01  sum_sqrt         7567    RNA:lung   RNA   lung  \n",
       "7568   0.01  sum_sqrt         7568    RNA:lung   RNA   lung  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_tasks(\n",
    "    model,\n",
    "    assay: Optional[str] = None,\n",
    "    description: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    ") -> Tuple[pd.DataFrame, List[int]]:\n",
    "    \"\"\"\n",
    "    Filter model tasks based on keywords (case-insensitive contains).\n",
    "    Returns filtered tasks dataframe and list of task indices.\n",
    "    \"\"\"\n",
    "    tasks = pd.DataFrame(model.data_params[\"tasks\"])\n",
    "    \n",
    "    mask = pd.Series([True] * len(tasks))\n",
    "    \n",
    "    if assay is not None:\n",
    "        mask &= tasks[\"assay\"].astype(str).str.lower().str.contains(assay.lower(), na=False)\n",
    "    \n",
    "    if description is not None:\n",
    "        mask &= tasks[\"description\"].astype(str).str.lower().str.contains(description.lower(), na=False)\n",
    "    \n",
    "    if name is not None:\n",
    "        mask &= tasks[\"name\"].astype(str).str.lower().str.contains(name.lower(), na=False)\n",
    "    \n",
    "    filtered_tasks = tasks[mask]\n",
    "    task_indices = filtered_tasks.index.tolist()\n",
    "    \n",
    "    return filtered_tasks, task_indices\n",
    "\n",
    "\n",
    "# Apply our filter\n",
    "filtered_tasks, task_indices = filter_tasks(\n",
    "    model,\n",
    "    assay=TASK_FILTER.get(\"assay\"),\n",
    "    description=TASK_FILTER.get(\"description\"),\n",
    "    name=TASK_FILTER.get(\"name\"),\n",
    ")\n",
    "\n",
    "print(f\"Filtered to {len(task_indices)} tasks\")\n",
    "print(f\"\\nSample of filtered tasks:\")\n",
    "filtered_tasks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-tasks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 3 matching tasks\n"
     ]
    }
   ],
   "source": [
    "# Validation: ensure we have tasks\n",
    "if len(task_indices) == 0:\n",
    "    raise ValueError(\n",
    "        f\"No tasks match the filter criteria: {TASK_FILTER}\\n\"\n",
    "        \"Please adjust TASK_FILTER in the configuration cell.\"\n",
    "    )\n",
    "\n",
    "print(f\"✓ Found {len(task_indices)} matching tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcf-header",
   "metadata": {},
   "source": [
    "## 6. Load and Validate VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-vcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF file not found: path/to/your/variants.vcf\n",
      "Creating synthetic demo variants around the gene TSS...\n",
      "\n",
      "Loaded 20 variants\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>rsID</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>pos_0based</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79823640</td>\n",
       "      <td>rs0</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>79823639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79848549</td>\n",
       "      <td>rs1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>79848548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79833614</td>\n",
       "      <td>rs2</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>79833613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79805376</td>\n",
       "      <td>rs3</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>79805375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr17</td>\n",
       "      <td>79821561</td>\n",
       "      <td>rs4</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>79821560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom       pos rsID ref alt  pos_0based\n",
       "0  chr17  79823640  rs0   T   G    79823639\n",
       "1  chr17  79848549  rs1   C   T    79848548\n",
       "2  chr17  79833614  rs2   C   T    79833613\n",
       "3  chr17  79805376  rs3   C   G    79805375\n",
       "4  chr17  79821561  rs4   T   C    79821560"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_vcf(vcf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load VCF file into a DataFrame.\n",
    "    Handles standard VCF format.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(vcf_path):\n",
    "        raise FileNotFoundError(f\"VCF file not found: {vcf_path}\")\n",
    "    \n",
    "    # Determine if gzipped\n",
    "    import gzip\n",
    "    opener = gzip.open if vcf_path.endswith('.gz') else open\n",
    "    \n",
    "    # Find header line to get column names\n",
    "    header_line = None\n",
    "    skip_rows = 0\n",
    "    \n",
    "    with opener(vcf_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('##'):\n",
    "                skip_rows += 1\n",
    "            elif line.startswith('#CHROM'):\n",
    "                header_line = line.strip().lstrip('#').split('\\t')\n",
    "                skip_rows += 1\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if header_line is None:\n",
    "        # Assume standard VCF columns\n",
    "        header_line = ['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO']\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(\n",
    "        vcf_path,\n",
    "        sep='\\t',\n",
    "        comment='#',\n",
    "        header=None,\n",
    "        names=header_line[:8],  # Only take standard columns\n",
    "        usecols=range(min(8, len(header_line))),\n",
    "    )\n",
    "    \n",
    "    # Standardize column names\n",
    "    df = df.rename(columns={\n",
    "        'CHROM': 'chrom',\n",
    "        'POS': 'pos',\n",
    "        'ID': 'rsID',\n",
    "        'REF': 'ref',\n",
    "        'ALT': 'alt',\n",
    "    })\n",
    "    \n",
    "    # Ensure chrom has 'chr' prefix\n",
    "    if not df['chrom'].iloc[0].startswith('chr'):\n",
    "        df['chrom'] = 'chr' + df['chrom'].astype(str)\n",
    "    \n",
    "    # Convert position to 0-based\n",
    "    df['pos_0based'] = df['pos'] - 1\n",
    "    \n",
    "    # Filter to SNPs only (single base ref and alt)\n",
    "    is_snp = (df['ref'].str.len() == 1) & (df['alt'].str.len() == 1)\n",
    "    n_before = len(df)\n",
    "    df = df[is_snp].reset_index(drop=True)\n",
    "    print(f\"Filtered {n_before - len(df)} non-SNP variants, {len(df)} SNPs remaining\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load VCF\n",
    "# For demo, create a synthetic VCF if file doesn't exist\n",
    "if not os.path.exists(VCF_FILE):\n",
    "    print(f\"VCF file not found: {VCF_FILE}\")\n",
    "    print(\"Creating synthetic demo variants around the gene TSS...\")\n",
    "    \n",
    "    # Create demo variants near the gene TSS\n",
    "    np.random.seed(42)\n",
    "    n_variants = 20\n",
    "    \n",
    "    demo_vcf = pd.DataFrame({\n",
    "        'chrom': [gene_info['chrom']] * n_variants,\n",
    "        'pos': gene_info['tss'] + np.random.randint(-100000, 100000, n_variants),\n",
    "        'rsID': [f'rs{i}' for i in range(n_variants)],\n",
    "        'ref': np.random.choice(['A', 'C', 'G', 'T'], n_variants),\n",
    "        'alt': np.random.choice(['A', 'C', 'G', 'T'], n_variants),\n",
    "    })\n",
    "    # Ensure ref != alt\n",
    "    for i in range(len(demo_vcf)):\n",
    "        while demo_vcf.loc[i, 'ref'] == demo_vcf.loc[i, 'alt']:\n",
    "            demo_vcf.loc[i, 'alt'] = np.random.choice(['A', 'C', 'G', 'T'])\n",
    "    \n",
    "    demo_vcf['pos_0based'] = demo_vcf['pos'] - 1\n",
    "    variants_df = demo_vcf\n",
    "else:\n",
    "    variants_df = load_vcf(VCF_FILE)\n",
    "\n",
    "print(f\"\\nLoaded {len(variants_df)} variants\")\n",
    "variants_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## 7. Validation Functions and Window Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output bins: 6144\n"
     ]
    }
   ],
   "source": [
    "def create_centered_window(center_pos: int, seq_len: int = SEQ_LEN) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Create a window of length seq_len centered on center_pos.\n",
    "    Returns (start, end) as 0-based coordinates.\n",
    "    \"\"\"\n",
    "    half_len = seq_len // 2\n",
    "    start = center_pos - half_len\n",
    "    end = start + seq_len\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def position_in_window(pos: int, window_start: int, window_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Convert genomic position to position within window.\n",
    "    Returns -1 if position is outside window.\n",
    "    \"\"\"\n",
    "    if window_start <= pos < window_end:\n",
    "        return pos - window_start\n",
    "    return -1\n",
    "\n",
    "\n",
    "def is_in_editable_region(pos_in_window: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if position within window is in the editable region\n",
    "    (i.e., will affect model output, not in cropped flanks).\n",
    "    \"\"\"\n",
    "    return EDITABLE_START <= pos_in_window < EDITABLE_END\n",
    "\n",
    "\n",
    "def get_output_bins_for_interval(\n",
    "    model, \n",
    "    interval_df: pd.DataFrame, \n",
    "    window_start: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get output bin indices for genomic intervals.\n",
    "    Uses model.input_intervals_to_output_bins.\n",
    "    \"\"\"\n",
    "    return model.input_intervals_to_output_bins(\n",
    "        intervals=interval_df,\n",
    "        start_pos=window_start\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_bins_in_output(\n",
    "    bin_df: pd.DataFrame,\n",
    "    n_output_bins: int\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate that all bins are within valid output range.\n",
    "    Returns (is_valid, message).\n",
    "    \"\"\"\n",
    "    min_bin = bin_df['start'].min()\n",
    "    max_bin = bin_df['end'].max()\n",
    "    \n",
    "    if min_bin < 0:\n",
    "        return False, f\"Bins extend before output start (min_bin={min_bin})\"\n",
    "    if max_bin > n_output_bins:\n",
    "        return False, f\"Bins extend beyond output end (max_bin={max_bin}, n_output={n_output_bins})\"\n",
    "    \n",
    "    return True, f\"All bins valid (range: {min_bin} - {max_bin}, output size: {n_output_bins})\"\n",
    "\n",
    "\n",
    "# Calculate number of output bins\n",
    "N_OUTPUT_BINS = LABEL_LEN // BIN_SIZE\n",
    "print(f\"Number of output bins: {N_OUTPUT_BINS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tss-centered-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS-Centered Window:\n",
      "  Window: chr17:79,539,538-80,063,826\n",
      "  TSS position: 79,801,682\n",
      "  TSS position in window: 262,144\n",
      "\n",
      "SNP Validation (TSS-centered):\n",
      "  SNPs in window: 20/20\n",
      "  SNPs in editable region: 19/20\n",
      "  ✓ 19 SNPs can be analyzed with TSS-centered window\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TSS-CENTERED VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create window centered on TSS\n",
    "tss_window_start, tss_window_end = create_centered_window(gene_info['tss'])\n",
    "\n",
    "print(\"TSS-Centered Window:\")\n",
    "print(f\"  Window: {gene_info['chrom']}:{tss_window_start:,}-{tss_window_end:,}\")\n",
    "print(f\"  TSS position: {gene_info['tss']:,}\")\n",
    "print(f\"  TSS position in window: {gene_info['tss'] - tss_window_start:,}\")\n",
    "\n",
    "# Check which SNPs fall within the editable region\n",
    "variants_df['pos_in_tss_window'] = variants_df['pos_0based'].apply(\n",
    "    lambda x: position_in_window(x, tss_window_start, tss_window_end)\n",
    ")\n",
    "variants_df['in_tss_window'] = variants_df['pos_in_tss_window'] >= 0\n",
    "variants_df['in_editable_region_tss'] = variants_df['pos_in_tss_window'].apply(\n",
    "    lambda x: is_in_editable_region(x) if x >= 0 else False\n",
    ")\n",
    "\n",
    "n_in_window = variants_df['in_tss_window'].sum()\n",
    "n_in_editable = variants_df['in_editable_region_tss'].sum()\n",
    "\n",
    "print(f\"\\nSNP Validation (TSS-centered):\")\n",
    "print(f\"  SNPs in window: {n_in_window}/{len(variants_df)}\")\n",
    "print(f\"  SNPs in editable region: {n_in_editable}/{len(variants_df)}\")\n",
    "\n",
    "if n_in_editable == 0:\n",
    "    print(\"  ⚠️ WARNING: No SNPs in editable region for TSS-centered analysis\")\n",
    "else:\n",
    "    print(f\"  ✓ {n_in_editable} SNPs can be analyzed with TSS-centered window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gene-bins-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene Exon Bins (TSS-centered window):\n",
      "   start   end\n",
      "0   2923  2929\n",
      "1   2909  2912\n",
      "2   2902  2905\n",
      "3   2896  2899\n",
      "4   2773  2881\n",
      "\n",
      "Validation: All bins valid (range: 2773 - 3073, output size: 6144)\n",
      "✓ All gene bins are within output field\n"
     ]
    }
   ],
   "source": [
    "# Get gene bins for TSS-centered window\n",
    "gene_exon_bins_tss = get_output_bins_for_interval(\n",
    "    model, \n",
    "    gene_info['exons'],\n",
    "    tss_window_start\n",
    ")\n",
    "\n",
    "print(\"Gene Exon Bins (TSS-centered window):\")\n",
    "print(gene_exon_bins_tss.head())\n",
    "\n",
    "# Validate bins are in output\n",
    "is_valid, msg = validate_bins_in_output(gene_exon_bins_tss, N_OUTPUT_BINS)\n",
    "print(f\"\\nValidation: {msg}\")\n",
    "if is_valid:\n",
    "    print(\"✓ All gene bins are within output field\")\n",
    "else:\n",
    "    print(f\"⚠️ WARNING: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-header",
   "metadata": {},
   "source": [
    "## 8. Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction functions defined.\n"
     ]
    }
   ],
   "source": [
    "import grelu.sequence.format\n",
    "\n",
    "def get_sequence_for_interval(\n",
    "    chrom: str,\n",
    "    start: int,\n",
    "    end: int,\n",
    "    genome: str = \"hg38\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get genomic sequence for an interval.\n",
    "    \"\"\"\n",
    "    interval_df = pd.DataFrame({\n",
    "        'chrom': [chrom],\n",
    "        'start': [start],\n",
    "        'end': [end],\n",
    "        'strand': ['+'],\n",
    "    })\n",
    "    \n",
    "    seqs = grelu.sequence.format.convert_input_type(\n",
    "        interval_df,\n",
    "        output_type=\"strings\",\n",
    "        genome=genome\n",
    "    )\n",
    "    return seqs[0]\n",
    "\n",
    "\n",
    "def mutate_sequence(seq: str, pos: int, new_base: str) -> str:\n",
    "    \"\"\"\n",
    "    Mutate a single position in the sequence.\n",
    "    \"\"\"\n",
    "    return seq[:pos] + new_base + seq[pos+1:]\n",
    "\n",
    "\n",
    "def predict_on_sequence(\n",
    "    model,\n",
    "    seq: str,\n",
    "    device: str = DEVICE\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run model prediction on a single sequence.\n",
    "    Returns array of shape (1, n_tasks, n_bins).\n",
    "    \"\"\"\n",
    "    preds = model.predict_on_seqs([seq], devices=device)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def aggregate_predictions(\n",
    "    preds: np.ndarray,\n",
    "    task_indices: List[int],\n",
    "    bin_indices: List[int],\n",
    "    task_aggfunc: str = TASK_AGGFUNC,\n",
    "    length_aggfunc: str = LENGTH_AGGFUNC,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Aggregate predictions over specified tasks and bins.\n",
    "    \n",
    "    Args:\n",
    "        preds: Array of shape (1, n_tasks, n_bins)\n",
    "        task_indices: List of task indices to use\n",
    "        bin_indices: List of bin indices to use\n",
    "        task_aggfunc: 'mean' or 'sum' for aggregating across tasks\n",
    "        length_aggfunc: 'mean' or 'sum' for aggregating across bins\n",
    "    \n",
    "    Returns:\n",
    "        Scalar aggregated prediction value\n",
    "    \"\"\"\n",
    "    # Select tasks and bins\n",
    "    selected = preds[0, task_indices, :][:, bin_indices]  # (n_selected_tasks, n_selected_bins)\n",
    "    \n",
    "    # Aggregate across bins (length axis)\n",
    "    if length_aggfunc == \"mean\":\n",
    "        agg_bins = np.mean(selected, axis=1)\n",
    "    else:\n",
    "        agg_bins = np.sum(selected, axis=1)\n",
    "    \n",
    "    # Aggregate across tasks\n",
    "    if task_aggfunc == \"mean\":\n",
    "        result = np.mean(agg_bins)\n",
    "    else:\n",
    "        result = np.sum(agg_bins)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "\n",
    "def get_all_bin_indices(bin_df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get all unique bin indices from start/end DataFrame.\n",
    "    Clips to valid output range.\n",
    "    \"\"\"\n",
    "    all_bins = set()\n",
    "    for _, row in bin_df.iterrows():\n",
    "        start = max(0, int(row['start']))\n",
    "        end = min(N_OUTPUT_BINS, int(row['end']))\n",
    "        if start < end:\n",
    "            all_bins.update(range(start, end))\n",
    "    return sorted(all_bins)\n",
    "\n",
    "\n",
    "print(\"Prediction functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-analysis-header",
   "metadata": {},
   "source": [
    "## 9. Main Analysis: Variant Effect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis function defined.\n"
     ]
    }
   ],
   "source": [
    "def analyze_variant(\n",
    "    variant_row: pd.Series,\n",
    "    models: Dict,\n",
    "    gene_info: Dict,\n",
    "    task_indices: List[int],\n",
    "    centering: str = \"tss\",  # \"tss\" or \"snp\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a single variant across all model replicates.\n",
    "    \n",
    "    Args:\n",
    "        variant_row: Row from variants DataFrame\n",
    "        models: Dict of model replicates\n",
    "        gene_info: Gene information dictionary\n",
    "        task_indices: Task indices to use\n",
    "        centering: \"tss\" or \"snp\"\n",
    "    \n",
    "    Returns:\n",
    "        Dict with ref/alt predictions for each replicate\n",
    "    \"\"\"\n",
    "    chrom = variant_row['chrom']\n",
    "    snp_pos = variant_row['pos_0based']\n",
    "    ref_allele = variant_row['ref'].upper()\n",
    "    alt_allele = variant_row['alt'].upper()\n",
    "    \n",
    "    # Determine window based on centering strategy\n",
    "    if centering == \"tss\":\n",
    "        center = gene_info['tss']\n",
    "    else:  # snp\n",
    "        center = snp_pos\n",
    "    \n",
    "    window_start, window_end = create_centered_window(center)\n",
    "    \n",
    "    # Position of SNP within window\n",
    "    snp_pos_in_window = snp_pos - window_start\n",
    "    \n",
    "    # Check if SNP is in editable region\n",
    "    if not (0 <= snp_pos_in_window < SEQ_LEN):\n",
    "        return None  # SNP outside window\n",
    "    \n",
    "    if centering == \"tss\" and not is_in_editable_region(snp_pos_in_window):\n",
    "        return None  # SNP in cropped region\n",
    "    \n",
    "    # Get gene bin indices for this window\n",
    "    gene_exon_bins = get_output_bins_for_interval(\n",
    "        list(models.values())[0],\n",
    "        gene_info['exons'],\n",
    "        window_start\n",
    "    )\n",
    "    bin_indices = get_all_bin_indices(gene_exon_bins)\n",
    "    \n",
    "    # Skip if no valid bins\n",
    "    if len(bin_indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get sequence\n",
    "    ref_seq = get_sequence_for_interval(chrom, window_start, window_end)\n",
    "    \n",
    "    # Verify reference allele matches\n",
    "    seq_at_pos = ref_seq[snp_pos_in_window].upper()\n",
    "    ref_match = seq_at_pos == ref_allele\n",
    "    \n",
    "    # Create alt sequence\n",
    "    alt_seq = mutate_sequence(ref_seq, snp_pos_in_window, alt_allele)\n",
    "    \n",
    "    # Run predictions for each replicate\n",
    "    ref_preds = []\n",
    "    alt_preds = []\n",
    "    \n",
    "    for rep, model in models.items():\n",
    "        ref_pred = predict_on_sequence(model, ref_seq)\n",
    "        alt_pred = predict_on_sequence(model, alt_seq)\n",
    "        \n",
    "        ref_agg = aggregate_predictions(ref_pred, task_indices, bin_indices)\n",
    "        alt_agg = aggregate_predictions(alt_pred, task_indices, bin_indices)\n",
    "        \n",
    "        ref_preds.append(ref_agg)\n",
    "        alt_preds.append(alt_agg)\n",
    "    \n",
    "    return {\n",
    "        'ref_preds': ref_preds,\n",
    "        'alt_preds': alt_preds,\n",
    "        'ref_mean': np.mean(ref_preds),\n",
    "        'alt_mean': np.mean(alt_preds),\n",
    "        'ref_match': ref_match,\n",
    "        'seq_at_pos': seq_at_pos,\n",
    "        'n_bins': len(bin_indices),\n",
    "        'centering': centering,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Analysis function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis on 20 variants...\n",
      "Using 4 model replicates and 2 centering strategies\n",
      "(Total of 8 predictions per allele per variant)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing variants:   0%|          | 0/20 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model replicates and 2 centering strategies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(models)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m predictions per allele per variant)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m SNP_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariants_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgene_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mrun_full_analysis\u001b[0;34m(variants_df, models, gene_info, task_indices)\u001b[0m\n\u001b[1;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(variants_df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(variants_df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing variants\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# TSS-centered analysis\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     tss_result \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_variant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# SNP-centered analysis\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     snp_result \u001b[38;5;241m=\u001b[39m analyze_variant(\n\u001b[1;32m     22\u001b[0m         row, models, gene_info, task_indices, centering\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[14], line 71\u001b[0m, in \u001b[0;36manalyze_variant\u001b[0;34m(variant_row, models, gene_info, task_indices, centering)\u001b[0m\n\u001b[1;32m     68\u001b[0m alt_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rep, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 71\u001b[0m     ref_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     alt_pred \u001b[38;5;241m=\u001b[39m predict_on_sequence(model, alt_seq)\n\u001b[1;32m     74\u001b[0m     ref_agg \u001b[38;5;241m=\u001b[39m aggregate_predictions(ref_pred, task_indices, bin_indices)\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mpredict_on_sequence\u001b[0;34m(model, seq, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_on_sequence\u001b[39m(\n\u001b[1;32m     31\u001b[0m     model,\n\u001b[1;32m     32\u001b[0m     seq: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     33\u001b[0m     device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEVICE\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Run model prediction on a single sequence.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Returns array of shape (1, n_tasks, n_bins).\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_seqs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fixed: devices -> device\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/lightning/__init__.py:709\u001b[0m, in \u001b[0;36mLightningModel.predict_on_seqs\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m    707\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 709\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/lightning/__init__.py:298\u001b[0m, in \u001b[0;36mLightningModel.forward\u001b[0;34m(self, x, logits)\u001b[0m\n\u001b[1;32m    295\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_input(x)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# forward() produces prediction (e.g. post-activation)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# unless logits=True, which is used in loss functions\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logits:\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/model/models.py:54\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m        Output tensor\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/model/trunks/borzoi.py:228\u001b[0m, in \u001b[0;36mBorzoiTrunk.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        Output tensor\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     x, y0, y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_tower(x)\n\u001b[1;32m    230\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet_tower(x, [y0, y1])\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/model/trunks/borzoi.py:99\u001b[0m, in \u001b[0;36mBorzoiConvTower.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mForward pass\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Output tensor\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m---> 99\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m x, y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m](x)\n\u001b[1;32m    101\u001b[0m x, y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/grelu/model/blocks.py:371\u001b[0m, in \u001b[0;36mStem.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        Output tensor\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m    373\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:371\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/mgc5166/.conda/envs/grelu_py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:366\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    356\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_full_analysis(\n",
    "    variants_df: pd.DataFrame,\n",
    "    models: Dict,\n",
    "    gene_info: Dict,\n",
    "    task_indices: List[int],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run analysis on all variants with both centering strategies.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(variants_df.iterrows(), total=len(variants_df), desc=\"Analyzing variants\"):\n",
    "        # TSS-centered analysis\n",
    "        tss_result = analyze_variant(\n",
    "            row, models, gene_info, task_indices, centering=\"tss\"\n",
    "        )\n",
    "        \n",
    "        # SNP-centered analysis\n",
    "        snp_result = analyze_variant(\n",
    "            row, models, gene_info, task_indices, centering=\"snp\"\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        record = {\n",
    "            'rsID': row['rsID'],\n",
    "            'chrom': row['chrom'],\n",
    "            'position': row['pos'],\n",
    "            'ref_allele': row['ref'],\n",
    "            'alt_allele': row['alt'],\n",
    "        }\n",
    "        \n",
    "        # TSS-centered results\n",
    "        if tss_result is not None:\n",
    "            record['tss_ref_exp'] = tss_result['ref_mean']\n",
    "            record['tss_alt_exp'] = tss_result['alt_mean']\n",
    "            record['tss_ref_match'] = tss_result['ref_match']\n",
    "        else:\n",
    "            record['tss_ref_exp'] = np.nan\n",
    "            record['tss_alt_exp'] = np.nan\n",
    "            record['tss_ref_match'] = np.nan\n",
    "        \n",
    "        # SNP-centered results\n",
    "        if snp_result is not None:\n",
    "            record['snp_ref_exp'] = snp_result['ref_mean']\n",
    "            record['snp_alt_exp'] = snp_result['alt_mean']\n",
    "            record['snp_ref_match'] = snp_result['ref_match']\n",
    "        else:\n",
    "            record['snp_ref_exp'] = np.nan\n",
    "            record['snp_alt_exp'] = np.nan\n",
    "            record['snp_ref_match'] = np.nan\n",
    "        \n",
    "        results.append(record)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate average across centering strategies (8 predictions total: 4 reps x 2 centering)\n",
    "    results_df['ref_gene_exp_level'] = results_df[['tss_ref_exp', 'snp_ref_exp']].mean(axis=1)\n",
    "    results_df['alt_gene_exp_level'] = results_df[['tss_alt_exp', 'snp_alt_exp']].mean(axis=1)\n",
    "    \n",
    "    # Calculate differences\n",
    "    results_df['diff'] = results_df['alt_gene_exp_level'] - results_df['ref_gene_exp_level']\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "print(f\"Running analysis on {len(variants_df)} variants...\")\n",
    "print(f\"Using {len(models)} model replicates and 2 centering strategies\")\n",
    "print(f\"(Total of {len(models) * 2} predictions per allele per variant)\\n\")\n",
    "\n",
    "SNP_annotations = run_full_analysis(\n",
    "    variants_df,\n",
    "    models,\n",
    "    gene_info,\n",
    "    task_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7482b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab37ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict_on_seqs in module grelu.lightning:\n",
      "\n",
      "predict_on_seqs(x: Union[str, List[str]], device: Union[str, int] = 'cpu') -> numpy.ndarray method of grelu.lightning.LightningModel instance\n",
      "    A simple function to return model predictions directly\n",
      "    on a batch of a single batch of sequences in string\n",
      "    format.\n",
      "    \n",
      "    Args:\n",
      "        x: DNA sequences as a string or list of strings.\n",
      "        device: Index of the device to use\n",
      "    \n",
      "    Returns:\n",
      "        A numpy array of predictions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.predict_on_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name: Tesla P100-PCIE-12GB\n"
     ]
    }
   ],
   "source": [
    "# First, verify PyTorch sees the GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "valid_diffs = SNP_annotations['diff'].dropna()\n",
    "\n",
    "if len(valid_diffs) > 1:\n",
    "    diff_mean = valid_diffs.mean()\n",
    "    diff_std = valid_diffs.std()\n",
    "    \n",
    "    if diff_std > 0:\n",
    "        SNP_annotations['normalized_diff'] = (\n",
    "            (SNP_annotations['diff'] - diff_mean) / diff_std\n",
    "        )\n",
    "    else:\n",
    "        SNP_annotations['normalized_diff'] = 0.0\n",
    "        print(\"Warning: Zero standard deviation in differences\")\n",
    "else:\n",
    "    SNP_annotations['normalized_diff'] = np.nan\n",
    "    print(\"Warning: Not enough valid differences for normalization\")\n",
    "\n",
    "print(f\"Normalization complete.\")\n",
    "print(f\"  Mean diff: {diff_mean:.6f}\")\n",
    "print(f\"  Std diff: {diff_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(f\"\\nSNP Annotation Results for {GENE_NAME}:\")\n",
    "print(f\"  Total variants analyzed: {len(SNP_annotations)}\")\n",
    "print(f\"  Variants with valid predictions: {SNP_annotations['diff'].notna().sum()}\")\n",
    "\n",
    "# Select columns for display\n",
    "display_cols = [\n",
    "    'rsID', 'chrom', 'position', 'ref_allele', 'alt_allele',\n",
    "    'ref_gene_exp_level', 'alt_gene_exp_level', 'diff', 'normalized_diff'\n",
    "]\n",
    "\n",
    "SNP_annotations[display_cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## 10. Visualization: Expression Delta Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-histograms",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw differences\n",
    "ax1 = axes[0]\n",
    "valid_diff = SNP_annotations['diff'].dropna()\n",
    "ax1.hist(valid_diff, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No effect')\n",
    "ax1.axvline(x=valid_diff.mean(), color='green', linestyle='-', linewidth=2, label=f'Mean: {valid_diff.mean():.4f}')\n",
    "ax1.set_xlabel('Expression Difference (Alt - Ref)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title(f'Raw Expression Deltas\\n{GENE_NAME} ({len(valid_diff)} SNPs)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Normalized differences\n",
    "ax2 = axes[1]\n",
    "valid_norm = SNP_annotations['normalized_diff'].dropna()\n",
    "ax2.hist(valid_norm, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No effect')\n",
    "\n",
    "# Add significance thresholds\n",
    "ax2.axvline(x=-2, color='purple', linestyle=':', linewidth=1.5, label='|Z| = 2')\n",
    "ax2.axvline(x=2, color='purple', linestyle=':', linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('Z-Score (Normalized Expression Difference)', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title(f'Z-Score Normalized Expression Deltas\\n{GENE_NAME} ({len(valid_norm)} SNPs)', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('expression_delta_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Raw diff - Mean: {valid_diff.mean():.6f}, Std: {valid_diff.std():.6f}\")\n",
    "print(f\"  Raw diff - Min: {valid_diff.min():.6f}, Max: {valid_diff.max():.6f}\")\n",
    "print(f\"  |Z| > 2: {(valid_norm.abs() > 2).sum()} variants ({100*(valid_norm.abs() > 2).mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TSS-centered vs SNP-centered predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reference expression comparison\n",
    "ax1 = axes[0]\n",
    "valid_mask = SNP_annotations['tss_ref_exp'].notna() & SNP_annotations['snp_ref_exp'].notna()\n",
    "ax1.scatter(\n",
    "    SNP_annotations.loc[valid_mask, 'tss_ref_exp'],\n",
    "    SNP_annotations.loc[valid_mask, 'snp_ref_exp'],\n",
    "    alpha=0.6\n",
    ")\n",
    "# Add diagonal line\n",
    "lims = [\n",
    "    min(ax1.get_xlim()[0], ax1.get_ylim()[0]),\n",
    "    max(ax1.get_xlim()[1], ax1.get_ylim()[1])\n",
    "]\n",
    "ax1.plot(lims, lims, 'r--', alpha=0.75, zorder=0)\n",
    "ax1.set_xlabel('TSS-Centered Prediction', fontsize=12)\n",
    "ax1.set_ylabel('SNP-Centered Prediction', fontsize=12)\n",
    "ax1.set_title('Reference Allele Expression\\n(TSS vs SNP centering)', fontsize=14)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Difference comparison\n",
    "ax2 = axes[1]\n",
    "tss_diff = SNP_annotations['tss_alt_exp'] - SNP_annotations['tss_ref_exp']\n",
    "snp_diff = SNP_annotations['snp_alt_exp'] - SNP_annotations['snp_ref_exp']\n",
    "valid_mask = tss_diff.notna() & snp_diff.notna()\n",
    "\n",
    "ax2.scatter(tss_diff[valid_mask], snp_diff[valid_mask], alpha=0.6)\n",
    "lims = [\n",
    "    min(ax2.get_xlim()[0], ax2.get_ylim()[0]),\n",
    "    max(ax2.get_xlim()[1], ax2.get_ylim()[1])\n",
    "]\n",
    "ax2.plot(lims, lims, 'r--', alpha=0.75, zorder=0)\n",
    "ax2.set_xlabel('TSS-Centered Effect', fontsize=12)\n",
    "ax2.set_ylabel('SNP-Centered Effect', fontsize=12)\n",
    "ax2.set_title('Variant Effect (Alt-Ref)\\n(TSS vs SNP centering)', fontsize=14)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('centering_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "if valid_mask.sum() > 2:\n",
    "    corr = np.corrcoef(tss_diff[valid_mask], snp_diff[valid_mask])[0, 1]\n",
    "    print(f\"Correlation between TSS and SNP-centered effects: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full results\n",
    "output_file = f'{GENE_NAME}_variant_effects.csv'\n",
    "SNP_annotations.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ANALYSIS COMPLETE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Gene: {GENE_NAME}\")\n",
    "print(f\"Chromosome: {gene_info['chrom']}\")\n",
    "print(f\"TSS: {gene_info['tss']:,}\")\n",
    "print(f\"\\nTask Filter: {TASK_FILTER}\")\n",
    "print(f\"Number of tasks used: {len(task_indices)}\")\n",
    "print(f\"\\nModel replicates: {MODEL_REPLICATES}\")\n",
    "print(f\"Centering strategies: TSS-centered, SNP-centered\")\n",
    "print(f\"\\nVariants analyzed: {len(SNP_annotations)}\")\n",
    "print(f\"Variants with valid predictions: {SNP_annotations['diff'].notna().sum()}\")\n",
    "print(f\"\\nTop variants by |Z-score|:\")\n",
    "top_variants = SNP_annotations.nlargest(5, 'normalized_diff', keep='first')[display_cols]\n",
    "print(top_variants.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-summary",
   "metadata": {},
   "source": [
    "## 12. Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-summary-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VALIDATION CHECKS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Reference allele matching\n",
    "tss_ref_match = SNP_annotations['tss_ref_match'].dropna()\n",
    "snp_ref_match = SNP_annotations['snp_ref_match'].dropna()\n",
    "\n",
    "print(f\"\\n1. Reference Allele Matching:\")\n",
    "if len(tss_ref_match) > 0:\n",
    "    print(f\"   TSS-centered: {tss_ref_match.sum()}/{len(tss_ref_match)} matched ({100*tss_ref_match.mean():.1f}%)\")\n",
    "if len(snp_ref_match) > 0:\n",
    "    print(f\"   SNP-centered: {snp_ref_match.sum()}/{len(snp_ref_match)} matched ({100*snp_ref_match.mean():.1f}%)\")\n",
    "\n",
    "# Check 2: SNPs in editable region (TSS-centered)\n",
    "print(f\"\\n2. SNPs in Editable Region (TSS-centered):\")\n",
    "n_valid_tss = SNP_annotations['tss_ref_exp'].notna().sum()\n",
    "print(f\"   {n_valid_tss}/{len(SNP_annotations)} variants ({100*n_valid_tss/len(SNP_annotations):.1f}%)\")\n",
    "\n",
    "# Check 3: Gene bins in output (TSS-centered)\n",
    "print(f\"\\n3. Gene Bins in Output Field:\")\n",
    "is_valid, msg = validate_bins_in_output(gene_exon_bins_tss, N_OUTPUT_BINS)\n",
    "status = \"✓\" if is_valid else \"✗\"\n",
    "print(f\"   {status} {msg}\")\n",
    "\n",
    "# Check 4: Missing predictions\n",
    "print(f\"\\n4. Missing Predictions:\")\n",
    "n_missing_tss = SNP_annotations['tss_ref_exp'].isna().sum()\n",
    "n_missing_snp = SNP_annotations['snp_ref_exp'].isna().sum()\n",
    "print(f\"   TSS-centered: {n_missing_tss} missing\")\n",
    "print(f\"   SNP-centered: {n_missing_snp} missing\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
